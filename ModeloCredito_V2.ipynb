{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24504\\4286282741.py:10: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('train.csv')\n",
      "e:\\ITESO\\9\\Modelos de Credito\\Proyecto 2\\cleanData.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df['Monthly_Inhand_Salary'] = df.groupby('Customer_ID')['Monthly_Inhand_Salary'].apply(lambda x: x.ffill().bfill())\n",
      "e:\\ITESO\\9\\Modelos de Credito\\Proyecto 2\\cleanData.py:28: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df['Credit_History_Age'] = df.groupby('Customer_ID')['Credit_History_Age'].apply(lambda x: x.ffill().bfill())\n",
      "e:\\ITESO\\9\\Modelos de Credito\\Proyecto 2\\cleanData.py:43: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df['Num_of_Delayed_Payment'] = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].apply(lambda x: x.ffill().bfill())\n",
      "e:\\ITESO\\9\\Modelos de Credito\\Proyecto 2\\cleanData.py:63: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df['Monthly_Balance'] = df.groupby('Customer_ID')['Monthly_Balance'].apply(lambda x: x.ffill().bfill())\n",
      "e:\\ITESO\\9\\Modelos de Credito\\Proyecto 2\\cleanData.py:68: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df['Num_Credit_Inquiries'] = df.groupby('Customer_ID')['Num_Credit_Inquiries'].apply(lambda x: x.ffill().bfill())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000/32000 [==============================] - 34s 1ms/step - loss: 0.8873 - accuracy: 0.6199 - val_loss: 0.9215 - val_accuracy: 0.6349\n",
      "Epoch 2/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 0.9153 - accuracy: 0.6339 - val_loss: 0.9301 - val_accuracy: 0.6491\n",
      "Epoch 3/500\n",
      "32000/32000 [==============================] - 34s 1ms/step - loss: 0.9214 - accuracy: 0.6405 - val_loss: 0.8570 - val_accuracy: 0.6357\n",
      "Epoch 4/500\n",
      "32000/32000 [==============================] - 34s 1ms/step - loss: 0.9456 - accuracy: 0.6400 - val_loss: 0.8527 - val_accuracy: 0.6423\n",
      "Epoch 5/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 0.9330 - accuracy: 0.6440 - val_loss: 1.1298 - val_accuracy: 0.6455\n",
      "Epoch 6/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.0174 - accuracy: 0.6430 - val_loss: 1.1018 - val_accuracy: 0.6465\n",
      "Epoch 7/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.0343 - accuracy: 0.6392 - val_loss: 1.8894 - val_accuracy: 0.6469\n",
      "Epoch 8/500\n",
      "32000/32000 [==============================] - 34s 1ms/step - loss: 1.0675 - accuracy: 0.6435 - val_loss: 1.1091 - val_accuracy: 0.6407\n",
      "Epoch 9/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.0684 - accuracy: 0.6420 - val_loss: 1.1360 - val_accuracy: 0.6332\n",
      "Epoch 10/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.1520 - accuracy: 0.6428 - val_loss: 1.4769 - val_accuracy: 0.6457\n",
      "Epoch 11/500\n",
      "32000/32000 [==============================] - 34s 1ms/step - loss: 0.9817 - accuracy: 0.6426 - val_loss: 0.9170 - val_accuracy: 0.6388\n",
      "Epoch 12/500\n",
      "32000/32000 [==============================] - 34s 1ms/step - loss: 1.0551 - accuracy: 0.6420 - val_loss: 1.5150 - val_accuracy: 0.6352\n",
      "Epoch 13/500\n",
      "32000/32000 [==============================] - 34s 1ms/step - loss: 1.2457 - accuracy: 0.6373 - val_loss: 1.1252 - val_accuracy: 0.6382\n",
      "Epoch 14/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.1596 - accuracy: 0.6370 - val_loss: 2.3233 - val_accuracy: 0.6361\n",
      "Epoch 15/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.1894 - accuracy: 0.6341 - val_loss: 1.7308 - val_accuracy: 0.6284\n",
      "Epoch 16/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.3513 - accuracy: 0.6337 - val_loss: 0.9397 - val_accuracy: 0.6379\n",
      "Epoch 17/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.0653 - accuracy: 0.6323 - val_loss: 1.5499 - val_accuracy: 0.6384\n",
      "Epoch 18/500\n",
      "32000/32000 [==============================] - 33s 1ms/step - loss: 1.0833 - accuracy: 0.6306 - val_loss: 1.4598 - val_accuracy: 0.6221\n",
      "Epoch 19/500\n",
      "32000/32000 [==============================] - 34s 1ms/step - loss: 1.0805 - accuracy: 0.6345 - val_loss: 1.1470 - val_accuracy: 0.6366\n",
      "Epoch 20/500\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 1.0541 - accuracy: 0.6338 - val_loss: 1.0068 - val_accuracy: 0.6339\n",
      "Epoch 21/500\n",
      "30698/32000 [===========================>..] - ETA: 1s - loss: 1.0829 - accuracy: 0.6322"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cleanData import clean\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load and clean the data\n",
    "data = pd.read_csv('train.csv')\n",
    "data = clean(data).drop('Credit_Score', axis=1)\n",
    "data.Score = data.Score.map({3:2, 2:1, 1:0})\n",
    "\n",
    "# Check for NaN values in the data\n",
    "assert not data.isnull().values.any(), \"NaN values found in data after cleaning\"\n",
    "\n",
    "# Split the data\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(data.drop('Score', axis=1), data.pop('Score'), test_size=0.2)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "xTrain = scaler.fit_transform(xTrain)\n",
    "xTest = scaler.transform(xTest)\n",
    "\n",
    "# One-hot encode the targets\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "yTrain_encoded = encoder.fit_transform(yTrain.values.reshape(-1, 1))\n",
    "yTest_encoded = encoder.transform(yTest.values.reshape(-1, 1))\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation=keras.layers.LeakyReLU(alpha=0.01), input_shape=(xTrain.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with gradient clipping\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=40)\n",
    "\n",
    "# Train the model with a larger batch size\n",
    "history = model.fit(\n",
    "    xTrain, \n",
    "    yTrain_encoded, \n",
    "    batch_size=2,  # Increased batch size\n",
    "    validation_split=0.2, \n",
    "    callbacks=[earlyStopping], \n",
    "    epochs=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Prueba1.keraas\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Prueba1.keraas\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Prueba1.keraas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
